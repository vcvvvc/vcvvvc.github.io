<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Server on Vcvc Blog</title>
    <link>https://6923403.github.io/tags/server/</link>
    <description>Recent content in Server on Vcvc Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 04 Apr 2021 18:21:09 +0800</lastBuildDate><atom:link href="https://6923403.github.io/tags/server/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Server sum</title>
      <link>https://6923403.github.io/post/server_sum/</link>
      <pubDate>Sun, 04 Apr 2021 18:21:09 +0800</pubDate>
      
      <guid>https://6923403.github.io/post/server_sum/</guid>
      <description>socket struct整理  https://6923403.github.io/post/socket_struct/  socket编程需要的头文件整理  https://6923403.github.io/post/socket_file/  socket function  https://6923403.github.io/post/socket/  epoll function  https://6923403.github.io/post/epoll_use/  sem function  https://6923403.github.io/post/sem/  server io actor  https://6923403.github.io/post/server_actor/  timer function  https://6923403.github.io/post/timer_function/   Linux Pthread 线程创建与使用 https://6923403.github.io/post/pthread/
C++11 thread https://6923403.github.io/post/cppthread/
 Unp note https://6923403.github.io/post/unp_note/
TCP_IP | 计算机网络 https://6923403.github.io/post/tcp_ip/
 exception class 异常类 https://6923403.github.io/post/cpp_exception/</description>
    </item>
    
    <item>
      <title>Timer function</title>
      <link>https://6923403.github.io/post/timer_function/</link>
      <pubDate>Sun, 07 Feb 2021 21:45:11 +0800</pubDate>
      
      <guid>https://6923403.github.io/post/timer_function/</guid>
      <description>非活跃，是指客户端（这里是浏览器）与服务器端建立连接后，长时间不交换数据，一直占用服务器端的文件描述符，导致连接资源的浪费。
  定时事件，是指固定一段时间之后触发某段代码，由该段代码处理一个事件，如从内核事件表删除事件，并关闭文件描述符，释放连接资源。
  定时器，是指利用结构体或其他形式，将多种定时事件进行封装起来。具体的，这里只涉及一种定时事件，即定期检测非活跃连接，这里将该定时事件与连接资源封装为一个结构体定时器。
  定时器容器，是指使用某种容器类数据结构，将上述多个定时器组合起来，便于对定时事件统一管理。具体的，项目中使用升序链表将所有定时器串联组织起来。
   Linux下提供了三种定时的方法:
 socket选项SO_RECVTIMEO和SO_SNDTIMEO SIGALRM信号 I/O复用系统调用的超时参数   Socket选项 struct timeval timeout; timeout.tv_sec = time; timeout.tv_usec = 0; socklen_t time_len = sizeof(timeout); ret = setsockopt(sockfd, SOL_SOCKET, SO_SNDTIMEO, &amp;amp;timeout, time_len); //发送数据超时 用返回值errno判断是否达到指定时间 (errno == SO_SNDTIMEO)  SIGALRM信号 sigaction结构体
struct sigaction { void (*sa_handler)(int); void (*sa_sigaction)(int, siginfo_t *, void *); sigset_t sa_mask; int sa_flags; void (*sa_restorer)(void); } sa_handler是一个函数指针，指向信号处理函数 sa_sigaction同样是信号处理函数，有三个参数，可以获得关于信号更详细的信息 sa_mask用来指定在信号处理函数执行期间需要被屏蔽的信号 sa_flags用于指定信号处理的行为 SA_RESTART，使被信号打断的系统调用自动重新发起 SA_NOCLDSTOP，使父进程在它的子进程暂停或继续运行时不会收到 SIGCHLD 信号 SA_NOCLDWAIT，使父进程在它的子进程退出时不会收到 SIGCHLD 信号，这时子进程如果退出也不会成为僵尸进程 SA_NODEFER，使对信号的屏蔽无效，即在信号处理函数执行期间仍能发出这个信号 SA_RESETHAND，信号处理之后重新设置为默认的处理方式 SA_SIGINFO，使用 sa_sigaction 成员而不是 sa_handler 作为信号处理函数 sa_restorer一般不使用</description>
    </item>
    
    <item>
      <title>Server actor</title>
      <link>https://6923403.github.io/post/server_actor/</link>
      <pubDate>Sun, 31 Jan 2021 22:19:30 +0800</pubDate>
      
      <guid>https://6923403.github.io/post/server_actor/</guid>
      <description>Reactor Reactor模式要求主线程（I/O处理单元，下同）只负责监听文件描述上是否有事件发生，有的话就立即将该事件通知工作线程（逻辑单元，下同）。除此之外，主线程不做任何其他实质性的工作。读写数据，接受新的连接，以及处理客户请求均在工作线程中完成。
Reactor工作流程∶
 主线程往 epoll内核事件表中注册 socket 上的读就绪事件。 主线程调用epoll_wait 等待socket上有数据可读。 当socket上有数据可读时，epoll_wait 通知主线程。主线程则将socket可读事件放入请求队列。 睡眠在请求队列上的某个工作线程被唤醒，它从socket读取数据，并处理客户请求，然后往 epoll 内核事件表中注册该 socket 上的写就绪事件。 主线程调用epoll_wait 等待socket可写。 当socket可写时，epoll_wait 通知主线程。主线程将socket可写事件放入请求队列。 睡眠在请求队列上的某个工作线程被唤醒，它往socket上写入服务器处理客户请求的结果。   Procactor 将多有I/O操作都交给主线程和内核来处理, 工作线程仅负责业务逻辑
Proactor工作流程∶
 主线程调用aio_read 函数向内核注册 socket 上的读完成事件，并告诉内核用户读缓冲区的位置，以及读操作完成时如何通知应用程序（这里以信号为例，详情请参考sigevent的 man 手册）。 主线程继续处理其他逻辑。 当socket上的数据被读入用户缓冲区后，内核将向应用程序发送一个信号，以通知应用程序数据已经可用。 应用程序预先定义好的信号处理函数选择一个工作线程来处理客户请求。工作线程处理完客户请求之后，调用 aio_write 函数向内核注册 socket 上的写完成事件，并告诉内核用户写缓冲区的位置，以及写操作完成时如何通知应用程序（仍然以信号为例）。 主线程继续处理其他逻辑。 当用户缓冲区的数据被写人 socket 之后，内核将向应用程序发送一个信号，以通知应用程序数据已经发送完毕。 应用程序预先定义好的信号处理函数选择一个工作线程来做善后处理，比如决定是否关闭 socket。   模拟Proactor 使用同步I/O模型（仍然以epoll_wait为例）模拟出的Proactor模式的工作流程∶
 主线程往 epoll 内核事件表中注册 socket 上的读就绪事件。 主线程调用 epoll_wait 等待 socket上有数据可读。 当 socket 上有数据可读时，epoll_wait通知主线程。主线程从socket循环读取数据，直到没有更多数据可读，然后将读取到的数据封装成一个请求对象并插入请求队列。 睡眠在请求队列上的某个工作线程被唤醒，它获得请求对象并处理客户请求，然后往 epoll 内核事件表中注册 socket上的写就绪事件。 主线程调用 epoll_wait 等待 socket 可写。 当 socket 可写时，epoll wait 通知主线程。主线程往 socket 上写人服务器处理客户请求的结果。    半同步/半异步 在I/O模型中，&amp;ldquo;同步&amp;quot;和&amp;quot;异步&amp;quot;区分的是内核向应用程序通知的是何种 I/O 事件（是就绪事件还是完成事件），以及该由谁来完成I/O读写（是应用程序还是内核）。在并发模式中，&amp;ldquo;同步&amp;quot;指的是程序完全按照代码序列的顺序执行∶&amp;quot;异步&amp;quot;指的是程序的执行需要由系统事件来驱动。常见的系统事件包括中断、信号等。</description>
    </item>
    
    <item>
      <title>epoll use</title>
      <link>https://6923403.github.io/post/epoll_use/</link>
      <pubDate>Thu, 27 Aug 2020 12:09:01 +0800</pubDate>
      
      <guid>https://6923403.github.io/post/epoll_use/</guid>
      <description>简介 #include &amp;lt;sys/epoll.h&amp;gt;
epoll与select
Epoll 没有最大并发连接的限制，上限是最大可以打开文件的数目 效率提升，epoll对于句柄事件的选择不是遍历的，是事件响应的，就是句柄上事件来就马上选择出来，不需要遍历整个句柄链表，因此效率非常高，内核将句柄用红黑树保存的，IO效率不随FD数目增加而线性下降。 内存拷贝， select让内核把 FD 消息通知给用户空间的时候使用了内存拷贝的方式，开销较大，但是Epoll 在这点上使用了共享内存的方式，这个内存拷贝也省略了。 相比于select，epoll最大的好处在于它不会随着监听fd数目的增长而降低效率。因为在内核中的select实现中，它是采用轮询来处理的，轮询的fd数目越多，自然耗时越多。 并且，在linux/posix_types.h头文件有这样的声明： #define __FD_SETSIZE 1024 表示select最多同时监听1024个fd，当然，可以通过修改头文件再重编译内核来扩大这个数目，但这似乎并不治本。 epoll是Linux内核为处理大批量文件描述符而作了改进的poll，是Linux下多路复用IO接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率。原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了。
 触发模式 epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。
 水平触发（LT）：默认工作模式，即当epoll_wait检测到某描述符事件就绪并通知应用程序时，应用程序可以不立即处理该事件；下次调用epoll_wait时，会再次通知此事件  //LevelTriggered(LT) //缺省工作方式，即默认的工作方式,支持blocksocket和no_blocksocket，错误率比较小。
 边缘触发（ET）： 当epoll_wait检测到某描述符事件就绪并通知应用程序时，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次通知此事件。（直到你做了某些操作导致该描述符变成未就绪状态了，也就是说边缘触发只在状态由未就绪变为就绪时只通知一次）。  //Edge Triggered(ET) //高速工作方式，错误率比较大，只支持no_block socket (非阻塞socket)
假设现在对方发送了2k的数据，而我们先读取了1k，然后这时调用了epoll_wait，如果是边沿触发ET，那么这个fd变成就绪状态就会从epoll 队列移除， 则epoll_wait 会一直阻塞，忽略尚未读取的1k数据; 而如果是水平触发LT，那么epoll_wait 还会检测到可读事件而返回，我们可以继续读取剩下的1k 数据。 总结: LT模式可能触发的次数更多, 一旦触发的次数多, 也就意味着效率会下降; 但这样也不能就说LT模式就比ET模式效率更低 因为ET的使用对编程人员提出了更高更精细的要求,一旦使用者编程水平不够, 那ET模式还不如LT模式。 ET模式仅当状态发生变化的时候才获得通知,这里所谓的状态的变化并不包括缓冲区中还有未处理的数据, 也就是说,如果要采用ET模式,需要一直read/write直到出错为止,很多人反映为什么采用ET模式只接收了一部分数据就再也得不到通知了,大多因为这样; 而LT模式是只要有数据没有处理就会一直通知下去的.  1. 创建一个epoll的句柄 int epoll_create(int size); 创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大。
这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。
2. 将被监听的描述符添加到epoll句柄或从epool句柄中删除或者对监听事件进行修改 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); //op为注册事件 epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。</description>
    </item>
    
  </channel>
</rss>
